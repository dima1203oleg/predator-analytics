#!/usr/bin/env python3
"""
Quick demo of LLM Council with Telegram-like queries
Shows how the system responds to natural language
"""
import asyncio
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ua-sources'))

from app.services.llm import llm_service


async def demo_telegram_query(query: str, mode: str = "fast"):
    """Simulate a Telegram user query"""
    print(f"\n{'='*70}")
    print(f"ğŸ“± USER: {query}")
    print(f"ğŸ¤– MODE: {mode}")
    print(f"{'='*70}\n")
    
    import time
    start = time.time()
    
    response = await llm_service.generate_with_routing(
        prompt=query,
        system="""Ğ¢Ğ¸ - AI Ğ°ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»Ñ–Ğ½Ğ½Ñ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼ Predator Analytics.
Ğ”Ğ¾Ğ¿Ğ¾Ğ¼Ğ°Ğ³Ğ°Ğ¹ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ñƒ Ğ·:
1. Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»Ñ–Ğ½Ğ½ÑĞ¼ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼ (ÑÑ‚Ğ°Ñ‚ÑƒÑ, Ñ€ĞµÑÑƒÑ€ÑĞ¸, Ğ»Ğ¾Ğ³Ğ¸)
2. Docker/Kubernetes ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»Ñ–Ğ½Ğ½Ñ  
3. SSH/Ngrok Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ
4. Ğ”ĞµĞ¿Ğ»Ğ¾Ğ¹ Ñ‚Ğ° Ğ¼Ğ¾Ğ½Ñ–Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
5. ĞŸĞ¾ÑˆÑƒĞº Ğ² ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¸Ñ… Ñ€ĞµÑ”ÑÑ‚Ñ€Ğ°Ñ…
6. Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñ– Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ Ğ· Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ñ–Ğ¹

Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾ Ñ‚Ğ° Ğ¿Ğ¾ ÑÑƒÑ‚Ñ– ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¾Ñ Ğ¼Ğ¾Ğ²Ğ¾Ñ.""",
        mode=mode
    )
    
    elapsed = time.time() - start
    
    if response.success:
        print(f"ğŸ¤– BOT RESPONSE:")
        print(f"{response.content}\n")
        print(f"ğŸ“Š Stats:")
        print(f"  â€¢ Provider: {response.provider}")
        print(f"  â€¢ Model: {response.model}")
        print(f"  â€¢ Latency: {response.latency_ms:.0f}ms ({elapsed:.1f}s total)")
        print(f"  â€¢ Tokens: {response.tokens_used}")
    else:
        print(f"âŒ ERROR: {response.error}")


async def demo_council_query(query: str):
    """Demonstrate Council mode for deep analysis"""
    print(f"\n{'='*70}")
    print(f"ğŸ“± USER: {query}")
    print(f"ğŸ¤– MODE: COUNCIL (Multi-model debate)")
    print(f"{'='*70}\n")
    
    print("â³ Stage 1: Gathering opinions from council members...")
    
    import time
    start = time.time()
    
    response = await llm_service.run_council(
        prompt=query,
        system="Ğ¢Ğ¸ - ĞµĞºÑĞ¿ĞµÑ€Ñ‚-Ğ°Ğ½Ğ°Ğ»Ñ–Ñ‚Ğ¸Ğº. Ğ”Ğ°Ğ²Ğ°Ğ¹ Ğ³Ğ»Ğ¸Ğ±Ğ¾ĞºÑ– Ñ‚Ğ° Ğ¾Ğ±Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–.",
        max_tokens=1000,
        enable_review=False  # Disable for demo speed
    )
    
    elapsed = time.time() - start
    
    if response.success:
        print(f"âœ… Council synthesis complete!\n")
        print(f"ğŸ¤– COUNCIL RESPONSE:")
        print(f"{response.content}\n")
        print(f"ğŸ“Š Stats:")
        print(f"  â€¢ Model: {response.model}")
        print(f"  â€¢ Total time: {elapsed:.1f}s")
        print(f"  â€¢ Latency: {response.latency_ms:.0f}ms")
    else:
        print(f"âŒ Council failed: {response.error}")


async def main():
    """Run demo"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘           ğŸ¤– Predator Analytics LLM Council Demo                â•‘
â•‘                 Telegram Natural Language Processing             â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # Demo 1: Simple queries (Fast mode)
    print("\nğŸ¯ DEMO 1: Simple Questions (Fast Mode)")
    print("â”" * 70)
    
    await demo_telegram_query(
        "ĞŸÑ€Ğ¸Ğ²Ñ–Ñ‚! Ğ¯Ğº Ñ‚ĞµĞ±Ğµ Ğ·Ğ²Ğ°Ñ‚Ğ¸?",
        mode="fast"
    )
    
    await asyncio.sleep(1)
    
    await demo_telegram_query(
        "ĞŸĞ¾ÑÑĞ½Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾ Ñ‰Ğ¾ Ñ‚Ğ°ĞºĞµ Docker?",
        mode="fast"
    )
    
    await asyncio.sleep(1)
    
    await demo_telegram_query(
        "Ğ¯Ğº Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑ ÑĞµÑ€Ğ²ĞµÑ€Ñƒ Ñ‡ĞµÑ€ĞµĞ· SSH?",
        mode="fast"
    )
    
    # Demo 2: Council mode for complex question
    print("\n\nğŸ¯ DEMO 2: Complex Analysis (Council Mode)")
    print("â”" * 70)
    
    await demo_council_query(
        "ĞŸĞ¾ÑÑĞ½Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ°Ğ³Ğ¸ Ñ‚Ğ° Ğ½ĞµĞ´Ğ¾Ğ»Ñ–ĞºĞ¸ Kubernetes Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ğ±Ñ–Ğ·Ğ½ĞµÑÑƒ. Ğ§Ğ¸ Ğ²Ğ°Ñ€Ñ‚Ğ¾ Ğ¹Ğ¾Ğ³Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ñ‚Ğ¸?"
    )
    
    print("\n\n" + "="*70)
    print("âœ… Demo completed!")
    print("="*70)
    print("""
ğŸ“‹ Summary:
  â€¢ Fast mode: 0.5-3 seconds - good for quick questions
  â€¢ Council mode: 60-90 seconds - best for complex analysis
  â€¢ All responses in Ukrainian
  â€¢ Multiple providers with automatic fallback
  â€¢ Ready for production Telegram bot!

ğŸ’¡ Key Features Demonstrated:
  âœ… Natural language processing
  âœ… Fast single-model responses  
  âœ… Multi-model council debate
  âœ… Ukrainian language support
  âœ… Tech-focused responses
  âœ… Provider fallback working

ğŸš€ System is ready for Telegram integration!
""")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Demo interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
