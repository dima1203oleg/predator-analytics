# Predator v22 Automation Plan

–¶–µ –¥–µ—Ç–∞–ª—å–Ω–∏–π –ø–ª–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó CI/CD, Dockerfile –∫–æ–Ω–≤–µ—Ä—Å—ñ–π, —Ç–∞ –∑–∞–≥–∞–ª—å–Ω–æ—ó —Ä–µ–ª—ñ–∑–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –¥–ª—è `v22.0`. –ü–ª–∞–Ω–∏ —Ç–∞ –∫—Ä–æ–∫–∏ –≤–∏–∫–æ–Ω–∞–Ω—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –ø–æ—Ç–æ—á–Ω–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ñ–≤ —É `implementation_v22` —Ç–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é.

## –¶—ñ–ª—ñ ‚úÖ

- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É–≤–∞—Ç–∏ Dockerfile –¥–ª—è –≤—Å—ñ—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤ (Python: 3.11) –∑ multi-stage builds —Ç–∞ distroless runtime, –∑–∞ –≤–∏–Ω—è—Ç–∫–æ–º GPU-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ–≤.
- –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ CI/CD: —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è (tox/pytest), —Å—Ç–∞—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ (ruff/black/mypy), —Å–∫–ª–∞–¥–∞–Ω–Ω—è –æ–±—Ä–∞–∑—ñ–≤, —Å–∫–∞–Ω –±–µ–∑–ø–µ–∫–∏ (Trivy/Clair), –ø—É—à —É GHCR, —Ç–∞ —ñ–Ω—ñ—Ü—ñ–∞—Ü—ñ—é ArgoCD sync.
- –î–æ–¥–∞—Ç–∏ e2e —Ç–µ—Å—Ç–∏ –≤ CI –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—ñ –æ–±—Ä–∞–∑—ñ–≤ —É —Ç–µ—Å—Ç–æ–≤–æ–º—É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ (kind/docker-compose).
- –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ release workflow: —Ç–µ–≥—É–≤–∞–Ω–Ω—è, —Ä–µ–ª—ñ–∑, –æ–Ω–æ–≤–ª–µ–Ω–Ω—è Helm values, PR –¥–ª—è ArgoCD (App-of-Apps), —Ç–∞ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—É –∞–≤—Ç–æ—Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—é.

## –û–±–º–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞

- Python >=3.11 (—Ç—ñ–ª—å–∫–∏ `3.11-slim` —É builder stage).
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ `gcr.io/distroless/python3` –¥–ª—è non-GPU runtimes.
- GPU images (CUDA) –∑–∞–ª–∏—à–∏—Ç–∏ –∑ –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–º–∏ CUDA –±–∞–∑–æ–≤–∏–º–∏ –æ–±—Ä–∞–∑–∞–º–∏; –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ `nvidia/cuda` –∞–±–æ `nvidia/cuda:XX.YY-devel-ubuntu..`.
- Image push ‚Äî —Ç—ñ–ª—å–∫–∏ –∑–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Å–µ–∫—Ä–µ—Ç—É `GHCR_PAT` —Ç–∞ –¥–æ–∑–≤–æ–ª—ñ–≤.

---

## Discovery: —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ñ–≤ (—Ç–µ—Ö–Ω—ñ—á–Ω—ñ –∫—Ä–æ–∫–∏) üîé

1. –ó–Ω–∞–π—Ç–∏ —É—Å—ñ Dockerfiles:

- `frontend/`, `ua-sources/`, `predator-frontend_22/` —Ç–∞ —ñ–Ω—à—ñ.

2. –í–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø –æ–±—Ä–∞–∑—É: Python, Node, CUDA, Misc.
3. –ü–æ–∑–Ω–∞—á–∏—Ç–∏ –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ: system dependencies (`libpq-dev`, `gcc`, `libmkl`), GPU runtime, non-root user setup.

### –†–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π - –∑–Ω–∞–π–¥–µ–Ω—ñ Dockerfile —Ç–∞ CI jobs (—Ä–µ–∑—É–ª—å—Ç–∞—Ç —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó)

- Dockerfile (backend): `ua-sources/Dockerfile` ‚Äî builder: `python:3.11-slim`, runtime: `gcr.io/distroless/python3` (multi-stage) ‚úÖ
- Dockerfile (frontend): `frontend/Dockerfile` ‚Äî builder: `node:18-alpine`, runtime: `nginx:alpine` (multi-stage static SPA) ‚úÖ
- Dockerfile (mlflow server): `infra/mlflow/Dockerfile` ‚Äî base: `ghcr.io/mlflow/mlflow:latest`; custom additions for `psycopg2-binary` & `boto3` ‚úÖ

- CI workflows that build & push images:
  - `.github/workflows/ci.yml` ‚Äî tests + validate Dockerfile + optional GHCR push; uses buildx and cache from `type=gha`.
  - `.github/workflows/ci-cd-pipeline.yml` ‚Äî build/test pipeline that also pushes to GHCR and updates Helm values (umbrella).
  - `.github/workflows/build-nvidia.yml` ‚Äî build & push and update `environments/nvidia/values.yaml` and commit.
  - `.github/workflows/deploy-nvidia-self-hosted.yml` ‚Äî deploy to NVIDIA cluster (self-hosted runner) and applies ArgoCD manifests + triggers sync.
  - Additional deploy workflows: `deploy-mac.yml`, `deploy-oracle.yml`, `deploy-prod.yml` ‚Äî each includes builds and environment updates for their target profiles.

### Observations

- Backend Dockerfile already conforms to multi-stage + distroless runtime and uses Python 3.11 ‚Äî no immediate conversion necessary.
- Few other Dockerfiles (frontend, mlflow) require specific handling and templates are appropriate for their runtime (Node / custom mlflow runtime).
- CI already includes many of the desired patterns (matrix tests, buildx, optional GHCR push). Improvements: digest-based tagging, helm value updates via PRs, explicit scan gating on severity, caching tune-up, and e2e test gating.

## –ö–æ–Ω–≤–µ—Ä—Å—ñ—è Dockerfile (–ø—Ä–∏–∫–ª–∞–¥ —à–∞–±–ª–æ–Ω—É) üõ†Ô∏è

- –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ö–µ–º–∞:
  - Stage `builder` –Ω–∞ `python:3.11-slim`
  - Stage `runtime` –Ω–∞ `gcr.io/distroless/python3` (–¥–ª—è non-GPU)
  - –ö–ª—é—á—ñ: –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ –æ—Ç–æ—á–µ–Ω–Ω—è, —Å—Ç–∞—Ç–∏—á–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ, –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è –ª–∏—à–µ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
  - –ü—Ä–∏–∫–ª–∞–¥ (–ø—Å–µ–≤–¥–æ):

```Dockerfile
FROM python:3.11-slim AS builder
WORKDIR /app
RUN apt-get update && apt-get install -y build-essential gcc libpq-dev --no-install-recommends && rm -rf /var/lib/apt/lists/*
COPY requirements.txt ./
RUN python -m venv /opt/venv && /opt/venv/bin/pip install --upgrade pip && /opt/venv/bin/pip install -r requirements.txt
COPY . .
RUN /opt/venv/bin/python -m pip install --no-deps -e .  # optional: editable for packages

FROM gcr.io/distroless/python3
WORKDIR /app
COPY --from=builder /opt/venv /opt/venv
COPY --from=builder /app /app
ENV PATH="/opt/venv/bin:$PATH"
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

- GPU images: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ CUDA –±–∞–∑—É (–æ–±–≥–æ–≤–æ—Ä–∏—Ç–∏ —à–∞–±–ª–æ–Ω –æ–∫—Ä–µ–º–æ).

## CI: Workflows (–∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞) ‚öôÔ∏è

1. `ci.yml` ‚Äî —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ linting (tox via matrix: py311, py312).
2. `build-images.yml` ‚Äî job matrix –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ—Ä–≤—ñ—Å:

- Build + scan (Trivy/Clair)
- Push to GHCR if `GHCR_PAT` present
- Cache: use buildx with action/cache and GitHub-registry cache
- Tagging: `sha`, `latest`, `v{tag}` (from release tag), `date`

3. `release.yml` ‚Äî on tag push:

- Run build-images with `vX.Y.Z` tags
- Update Helm `values.yaml` with new images and create PR in `deployment` repo (or update in repo with `gh` CLI)
- Optional: create GH Release and attach artifacts

4. `deploy.yml` ‚Äî on PR/merge to `main` or after successful release (manual or scheduled):

- Trigger ArgoCD sync via CLI (safest) or API (bearer token)

## Helm Values update & ArgoCD sync (safe approach) üîÅ

- After building & pushing images on release, update `helm/charts/umbrella/values.yaml` with the new tags and image digests.
- Create PR back into the `helm-charts` repo or the deployment manifest repo with the updated values; this will be reviewed and merged.
- When merged, ArgoCD will pick up changes and sync; optionally, CI can call `argocd app sync` using a service account token.

## CI Improvements - recommendations ‚öôÔ∏è

- Standardized image tagging: include `sha` and `semver` tag. Prefer immutable image digests (sha256) when updating Helm values.
- Use Docker Buildx with `cache-from` and `cache-to` GH Actions cache to speed up builds.
- Use `docker/build-push-action` to push and then read the pushed image digest (or use skopeo/registry API) ‚Äî update Helm values with the digest (prefixed `@sha256:`) rather than mutable tags.
- Use `yq` or `kubectl set image` to safely update YAML values rather than `sed` for maintainability.
- Implement PR-based updates for Helm values instead of direct commits by CI to main (unless we have a dedicated bot account).
- Make pushes conditional on `GITHUB_REF`/`github.event_name` & PR status and use `GHCR_PAT` for GHCR push permissions.
- Add a job that runs Trivy with higher severity gating for release builds to prevent accidental release of high/critical vulns.

## E2E testing & validation ‚úÖ

- Implement GitHub Actions job `e2e-test` that:
  - Spins up a test cluster (kind) or uses `docker-compose` to run minimal infra (postgres, minio, qdrant/opensearch)
  - Deploy built images to that environment
  - Run a small suite of smoke tests that validate readiness and endpoints
  - Block image pushes & releases if e2e fails

## Security & scanning üîí

- CI job: run Trivy (or Snyk) on images to detect vulns; fail pipeline if critical vulnerabilities found.
- Add an image size threshold policy (70% of target environment cap) as a warning in a job; configurable via CI input.

## Rollout plan & rollback strategy üö®

- Canary approach: deploy to a canary namespace first (e.g., `canary`); run smoke tests; if pass, rollout to `prod`.
- Rollback managed by ArgoCD: revert Helm values or use ArgoCD `rollback` if release fails.

## Migration tasks & dev experience üß≠

- Create `implementation_v22/scripts/dockerfile-template.sh` to generate standardized Dockerfile.
- Add `implementation_v22/scripts/run-lint-and-tests.sh` to run local tests and `tox` via Docker fallback.
- Document the process in `implementation_v22/README.md` and `DEVELOPER_SETUP.md`.

---

## Deliverables & Timeline

1. Inventory and CI scan (1 day)
2. Dockerfile refactor for backend & policy engine (1 day)
3. CI workflows to build & push images, update helm values (2 days)
4. e2e validations and ArgoCD sync PR flow (2 days)
5. Security scanning & size checks (1 day)
6. Documentation & Dev tooling updates (1 day)

## Next Steps (Immediate)

1. Inventory Dockerfiles and CI jobs to identify scope of work.
2. Convert the `backend` Dockerfile (`ua-sources/Dockerfile`) into the standard template and validate in CI.
3. Add the CI build & push job for the backend and run a dry-run build in a testing environment.
4. Add `build-and-release.yml` workflow to build images, get digests, run Trivy scan, e2e tests and open a PR updating Helm umbrella values to use the image digest and create PR.
5. Update Helm templates to prefer `image.digest` if present (already done for backend and celery).
6. Implement canary deployments and smoke tests in CI (optional):

- Add a `values-canary.yaml` with minimal replicas and canary host.
- Create `deploy-canary.sh` to deploy `helm/predator-umbrella` into `canary` namespace using a base64-encoded `KUBECONFIG` secret (named `KUBECONFIG_BASE64`).
- Add CI job in `build-and-release.yml` to deploy to canary and run simple curl-based smoke tests inside the cluster; only executes if `KUBECONFIG_BASE64` secret is present.

---

If –≤–∏ –∑–∞—Ç–≤–µ—Ä–¥–∂—É—î—Ç–µ —Ü–µ–π –ø–ª–∞–Ω ‚Äî —è –ø–æ—á–∏–Ω–∞—é –∑ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó Dockerfiles —Ç–∞ CI workflows, –ø–æ—Ç—ñ–º –≤–∏–∫–æ–Ω–∞—é —Ç–µ—Å—Ç–æ–≤—É –∫–æ–Ω–≤–µ—Ä—Å—ñ—é `ua-sources/Dockerfile` —Ç–∞ –¥–æ–¥–∞–º –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π CI job (–±—É–¥—É–≤–∞–Ω–Ω—è, —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è, –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –ø—É—à).

–§–∞–π–ª —Å—Ç–≤–æ—Ä–µ–Ω–æ: `implementation_v22/automation-plan.md`.
