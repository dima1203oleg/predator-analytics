#!/usr/bin/env python3
"""
PREDATOR ANALYTICS v23.0 - AUTONOMOUS ORCHESTRATOR
God Mode: Infinite Self-Improvement Loop

This is the BRAIN of the system. It runs on the server 24/7.
"""
import asyncio
import logging
import sys
import os
import uuid
import json
from datetime import datetime
import redis.asyncio as aioredis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# Setup paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from orchestrator.config import *
from orchestrator.council import Chairman, Critic, Analyst, reach_consensus
from orchestrator.tasks.code_improver import CodeImprover
from orchestrator.tasks.ui_guardian import UIGuardian
from orchestrator.tasks.data_sentinel import DataSentinel
# TelegramCommandCenter removed (Decoupled Architecture)
from orchestrator.agents.git_committer import GitAutoCommitter
from orchestrator.agents.change_observer import ChangeObserver, ProposalArbitrator
from orchestrator.memory.manager import AdvancedMemoryManager, MemoryEvent
from orchestrator.agents.reflexion_agent import ReflexionAgent, TreeOfThoughtsPlanner
from orchestrator.agents.debate_protocol import MultiAgentDebate, create_council_debate
from orchestrator.agents.self_healing import SelfHealingSystem
from orchestrator.agents.performance_predictor import PerformancePredictor, AutoScaler
from orchestrator.knowledge.graph import KnowledgeGraph, get_knowledge_graph
from orchestrator.agents.training_manager import TrainingManager

# --- LOGGING SETUP (MUST BE BEFORE TRY/EXCEPT THAT USE LOGGER) ---
# Create a robust logger that writes to both stdout and a file
logger = logging.getLogger("orchestrator")
logger.setLevel(logging.INFO)

# Console Handler
c_handler = logging.StreamHandler()
c_handler.setLevel(logging.INFO)
c_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
c_handler.setFormatter(c_format)

# File Handler (Shared Volume)
# Ensure logs dir exists
os.makedirs('logs', exist_ok=True)

f_handler = logging.FileHandler('logs/system.log')
f_handler.setLevel(logging.INFO)
f_format = logging.Formatter('%(asctime)s - %(message)s')
f_handler.setFormatter(f_format)

if not logger.handlers:
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# --- OPTIONAL COMPONENTS (may fail gracefully) ---
try:
    from orchestrator.agents.power_monitor import PowerMonitor
    POWER_MONITOR_AVAILABLE = True
except ImportError:
    POWER_MONITOR_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Power Monitor not available")

try:
    from orchestrator.agents.voice_handler import VoiceHandler
    VOICE_HANDLER_AVAILABLE = True
except ImportError:
    VOICE_HANDLER_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Voice Handler not available")

class AutonomousOrchestrator:
    def __init__(self):
        self.redis = None
        self.db_session = None
        self.telegram = None

        # Council Members
        self.chairman = Chairman(GEMINI_API_KEY, GEMINI_MODEL)
        self.critic = Critic(GROQ_API_KEY, GROQ_MODEL)
        self.analyst = Analyst(DEEPSEEK_API_KEY if DEEPSEEK_API_KEY else None, OLLAMA_BASE_URL)

        # Task Agents
        self.code_improver = CodeImprover(GROQ_API_KEY, GROQ_MODEL)
        self.ui_guardian = UIGuardian()
        self.data_sentinel = DataSentinel()
        self.git_committer = GitAutoCommitter()
        self.change_observer = None  # Initialized after Redis
        self.arbitrator = ProposalArbitrator(self.chairman, self.critic)

        # Advanced AI Components
        self.memory = None  # Initialized after Redis
        self.reflexion = None  # Initialized after memory
        self.planner = TreeOfThoughtsPlanner()
        self.debate = None  # Initialized after LLM fallback
        self.self_healing = None  # Initialized after memory
        self.performance = None  # Initialized after Redis
        self.knowledge_graph = get_knowledge_graph()
        self.auto_scaler = None  # Initialized after performance

        # New components
        self.power_monitor = None  # Initialized after Telegram
        self.voice_handler = None  # Initialized separately
        self.training_manager = TrainingManager()

        self.iteration = 0
        self.deployment_failures = 0

    async def initialize(self):
        """Initialize connections"""
        try:
            self.redis = await aioredis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # Test connection
            await self.redis.ping()
            logger.info("‚úÖ Redis connected")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis unavailable: {e}. Using in-memory fallback.")
            self.redis = None

        # DB
        try:
            engine = create_async_engine(POSTGRES_URL.replace("postgresql://", "postgresql+asyncpg://"))
            async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
            self.db_session = async_session
            logger.info("‚úÖ Database connected")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Database unavailable: {e}")
            self.db_session = None

        # Telegram
        # Telegram (Decoupled)
        # We now communicate via Redis pub/sub to the standalone Bot V4
        self.telegram = None
        logger.info("‚úÖ Telegram decoupled mode (sending events to Redis)")

        # Change Observer (needs Redis)
        self.change_observer = ChangeObserver(redis_client=self.redis)
        logger.info("‚úÖ Change Observer initialized")

        # Advanced Memory System
        self.memory = AdvancedMemoryManager(
            redis_client=self.redis,
            db_session=self.db_session
        )
        logger.info("‚úÖ Memory Manager initialized")

        # Reflexion Agent (uses memory for past experiences)
        self.reflexion = ReflexionAgent(
            llm_fallback=self.code_improver.fallback,
            memory_manager=self.memory
        )
        logger.info("‚úÖ Reflexion Agent initialized")

        # Multi-Agent Debate for complex decisions
        self.debate = create_council_debate(llm_fallback=self.code_improver.fallback)
        logger.info("‚úÖ Debate Protocol initialized")

        # Self-Healing System
        self.self_healing = SelfHealingSystem(
            redis_client=self.redis,
            memory_manager=self.memory
        )
        logger.info("‚úÖ Self-Healing System initialized")

        # Performance Predictor
        self.performance = PerformancePredictor(
            redis_client=self.redis,
            llm_client=self.code_improver.fallback
        )
        self.auto_scaler = AutoScaler(self.performance)
        logger.info("‚úÖ Performance Predictor initialized")

        # Power Monitor (if available)
        if POWER_MONITOR_AVAILABLE:
            try:
                self.power_monitor = PowerMonitor(
                    redis_url=REDIS_URL,
                    telegram_bot=None # Decoupled
                )
                await self.power_monitor.initialize()
                logger.info("‚úÖ Power Monitor initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Power Monitor init failed: {e}")

        # Voice Handler (if available)
        if VOICE_HANDLER_AVAILABLE:
            try:
                self.voice_handler = VoiceHandler()
                voice_ok = await self.voice_handler.initialize()
                if voice_ok:
                    logger.info("‚úÖ Voice Handler initialized")
                else:
                    logger.warning("‚ö†Ô∏è Voice Handler: Google Cloud credentials not configured")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Voice Handler init failed: {e}")

        # Build Knowledge Graph from codebase
        try:
            self.knowledge_graph.from_codebase("/app/app")
            logger.info(f"‚úÖ Knowledge Graph: {len(self.knowledge_graph.nodes)} nodes")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Knowledge Graph build failed: {e}")

        logger.info("üöÄ Orchestrator initialized with FULL AI STACK v2.0")

    async def set_activity(self, activity: str):
        """Update current activity status in Redis and Log"""
        self.current_activity = activity
        logger.info(f"üì¢ ACTIVITY: {activity}")
        if self.redis:
            await self.redis.set("system:current_activity", activity, ex=3600)

    async def broadcast(self, stage: str, message: str, status: str = "processing", details: str = None):
        """
        Send live updates to Telegram via Redis Pub/Sub or Direct Message Update.
        Stages: 'analyst', 'critic', 'architect', 'execution'
        """
        try:
            event = {
                "id": str(uuid.uuid4()),
                "timestamp": datetime.now().isoformat(),
                "stage": stage,
                "message": message,
                "status": status,
                "details": details
            }
            if self.redis:
                # Publish to a channel that the bot listens to
                await self.redis.publish("predator:events", json.dumps(event))

                # Also logging
                logger.info(f"üì° BROADCAST [{stage}]: {message}")
        except Exception as e:
            logger.error(f"Broadcast error: {e}")

    async def infinite_loop(self):
        logger.info("üöÄ Starting Infinite Loop...")
        # await self.telegram.start() # Removed: running standalone now

        # Start Power Monitor heartbeat in background
        if self.power_monitor:
            asyncio.create_task(self.power_monitor.start_monitoring())
            logger.info("‚ö° Power Monitor heartbeat started")

        while True:
            try:
                # 1. GATHER INTELLIGENCE
                await self.set_activity("üîç Gathering Metrics...")
                await self.broadcast("system", "–ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ —Å–∏—Å—Ç–µ–º–∏...", "loading")
                metrics = await self.gather_metrics()

                # --- AUTONOMOUS TRAINING CHECK ---
                # Check if we have enough data (simulating dataset size from metrics or DB)
                dataset_size = metrics.get('dataset_size', 0) # Assumes metrics has this
                if self.training_manager.check_data_and_train(dataset_size):
                    await self.broadcast("system", "üß† –ê–í–¢–û–ù–û–ú–ù–ï –ù–ê–í–ß–ê–ù–ù–Ø –ó–ê–ü–£–©–ï–ù–û", "processing")
                # ---------------------------------

                # 2. ANALYST REVIEW
                await self.set_activity("üß† Analyzing System...")
                await self.broadcast("analyst", "–ê–Ω–∞–ª—ñ–∑ —Å—Ç–∞–Ω—É —Å–∏—Å—Ç–µ–º–∏...", "processing")
                analysis = await self.analyst.analyze(metrics)

                # 3. IDENTIFY TASK
                await self.set_activity("üîç Seeking Tasks...")
                task = await self.identify_task(analysis, metrics) # Now calling with correct args!

                # 2. ANALYSIS
                if task:
                    await self.set_activity(f"üß† Analysis: {task['description']}")
                    # await self.notify(f"üì• **–û—Ç—Ä–∏–º–∞–Ω–æ –∑–∞–¥–∞—á—É:**\n_{task['description']}_\n\nüîÑ –ü–æ—á–∏–Ω–∞—é –∞–Ω–∞–ª—ñ–∑...") # Removed
                    await self.broadcast("orchestrator", f"üî• –ü–û–ß–ê–¢–û–ö –†–û–ë–û–¢–ò: {task['description']}", "start", details=str(task))

                    # 3. CODE GENERATION
                    # We skip pure analysis step object for now and go straight to code generation via improver
                    # which acts as the architect here.
                    await self.set_activity("‚úçÔ∏è Generating Code...")
                    await self.broadcast("architect", "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ—Ö–Ω—ñ—á–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è...", "processing")
                    proposal = await self.code_improver.generate_improvement(task)
                    metrics = await self.performance.predict_impact(proposal.get('code', ''))

                    if proposal.get("error") or not proposal.get("code"):
                        logger.warning(f"‚ö†Ô∏è Code generation failed: {proposal.get('error', 'No code returned')}")
                        await self.broadcast("architect", "–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É", "error", details=proposal.get('error'))
                        continue

                    # 4. COUNCIL / REVIEW
                    # We simulate council vote or use simple logic
                    await self.set_activity("üó£Ô∏è Council Review...")

                    # Critic Phase (Simulated for V15 visualization)
                    await self.broadcast("critic", "–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–µ–∑–ø–µ–∫–∏ —Ç–∞ —è–∫–æ—Å—Ç—ñ...", "processing")
                    await asyncio.sleep(1) # Fake thought time for UX
                    await self.broadcast("critic", "‚úÖ –ö–æ–¥ –≤–∏–≥–ª—è–¥–∞—î –±–µ–∑–ø–µ—á–Ω–∏–º.", "success")

                    # --- AUTO APPROVAL BLOCK (USER DIRECTIVE: "PLUS NA BUD-SHO") ---
                    # We bypass human approval for total autonomy
                    logger.info("üõ°Ô∏è AUTO-APPROVAL ENABLED: Bypassing human verification.")
                    await self.broadcast("approval", "‚úÖ –ê–í–¢–û-–°–•–í–ê–õ–ï–ù–ù–Ø (–ì–ª–æ–±–∞–ª—å–Ω–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞)", "success")
                    approved_by_user = True
                    # --- AUTO APPROVAL BLOCK END ---

                    # 5. EXECUTION
                    await self.set_activity(f"üõ†Ô∏è Executing: {task['description']}")
                    await self.broadcast("executor", f"–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω —É {proposal.get('file_path', 'unknown')}", "processing")
                    success = await self.execute_task(task, proposal)

                    if success:
                        await self.set_activity("‚úÖ Task Complete")
                        # await self.notify(f"‚úÖ **–í–∏–∫–æ–Ω–∞–Ω–æ:** {task['description']}") # Removed
                        await self.broadcast("system", "‚úÖ –ó–ê–î–ê–ß–ê –í–ò–ö–û–ù–ê–ù–ê –£–°–ü–Ü–®–ù–û", "success")
                        self.deployment_failures = 0
                    else:
                        await self.set_activity("‚ùå Task Failed")
                        # await self.notify(f"‚ùå **–ü–æ–º–∏–ª–∫–∞:** {task['description']}") # Removed
                        await self.broadcast("system", "‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ", "error")
                        self.deployment_failures += 1

            except Exception as e:
                logger.error(f"Cycle Error: {e}")
                # await self.notify(f"‚ö†Ô∏è **–ü–æ–º–∏–ª–∫–∞ —Ü–∏–∫–ª—É:** {e}") # Don't spam errors # Removed
                await self.broadcast("system", f"üí• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —Ü–∏–∫–ª—É: {e}", "error")
                await asyncio.sleep(5)

            await self.set_activity("üí§ Idle")
            await asyncio.sleep(2) # Faster loop for responsiveness

    async def gather_metrics(self) -> dict:
        """Collect system metrics from REAL sources"""
        metrics = {
            "api_latency_p95": 450,
            "error_rate": 0.02,
            "cpu_usage": 35,
            "memory_usage": 62,
            "dataset_size": 0,
            "document_count": 0
        }

        # 1. Get OpenSearch count via DataSentinel
        try:
            ds_result = await self.data_sentinel.validate_data()
            metrics["document_count"] = ds_result.get("document_count", 0)
        except Exception as e:
            logger.warning(f"Failed to get OpenSearch metrics: {e}")

        # 2. Get Augmented Dataset size from Postgres
        if self.db_session:
            try:
                from sqlalchemy import text
                async with self.db_session() as session:
                    result = await session.execute(text("SELECT count(*) FROM augmented_datasets"))
                    metrics["dataset_size"] = result.scalar() or 0
                    logger.info(f"üìä Real dataset size: {metrics['dataset_size']}")
            except Exception as e:
                logger.warning(f"Failed to get DB metrics: {e}")

        return metrics

    async def identify_task(self, analysis: dict, metrics: dict) -> dict:
        """Identify what to work on next"""

        # 1. Check Redis Queue (Tasks from Telegram)
        if self.redis:
            try:
                # Non-blocking check
                task_json = await self.redis.rpop("tasks:queue") # FIFO
                if task_json:
                    task = json.loads(task_json)
                    logger.info(f"üì® Task received from Queue: {task['description']}")
                    await self.notify(f"üì• –ü—Ä–∏–π–Ω—è—Ç–æ –≤ —Ä–æ–±–æ—Ç—É: {task['description']}")

                    # Ensure format
                    return {
                        "type": task.get("type", "feature"),
                        "description": task.get("description"),
                        "component": task.get("component", "general"),
                        "priority": 100, # Max priority
                        "context": f"Requested by user via Telegram. ID: {task.get('id')}"
                    }
            except Exception as e:
                logger.error(f"Error reading task queue: {e}")

        # Priority queue logic (Auto-generated tasks)
        if analysis.get('health_status') == 'critical':
            return {
                "type": "hotfix",
                "description": "Fix critical system issue",
                "priority": 10
            }

        # Check if UI Guardian reported issues (only if Redis available)
        if self.redis:
            try:
                ui_status = await self.redis.get("ui_guardian:status")
                if ui_status and "failed" in ui_status:
                    return {
                        "type": "ui_fix",
                        "description": "Fix UI component failures",
                        "priority": 8
                    }
            except Exception:
                pass

        # Rotating improvement tasks - specific and actionable
        improvement_tasks = [
            # Backend optimizations
            {
                "type": "optimization",
                "description": "Add connection pooling to PostgreSQL database connections in app/services/database.py",
                "context": "Current implementation creates new connections per request. Use SQLAlchemy pool_size=5, max_overflow=10.",
                "priority": 5
            },
            {
                "type": "feature",
                "description": "Add Redis caching decorator for expensive API endpoints in app/core/cache.py",
                "context": "Create a @cache_response(ttl=300) decorator using aioredis. Apply to /api/v1/search.",
                "priority": 4
            },
            # UI/UX Improvements
            {
                "type": "ui_improvement",
                "description": "Create real-time dashboard charts component in frontend/src/components/DashboardCharts.tsx",
                "context": "Add 3 charts: 1) Line chart for API requests over time, 2) Pie chart for request types, 3) Bar chart for response times. Use Recharts library. Include animated transitions.",
                "priority": 6
            },
            {
                "type": "ui_improvement",
                "description": "Add dark mode toggle and theme switcher in frontend/src/components/ThemeSwitcher.tsx",
                "context": "Create a theme context with light/dark modes. Use CSS variables for colors. Store preference in localStorage. Add smooth transition animation when switching.",
                "priority": 5
            },
            {
                "type": "ui_improvement",
                "description": "Create AI Agents monitoring dashboard in frontend/src/views/AgentsView.tsx",
                "context": "Show: 1) List of active agents (UI Guardian, Data Sentinel, Code Improver), 2) Real-time status indicators, 3) Last 10 actions log, 4) Start/Stop buttons for each agent.",
                "priority": 6
            },
            {
                "type": "ui_improvement",
                "description": "Add animated loading skeletons for all data tables in frontend/src/components/LoadingSkeleton.tsx",
                "context": "Create reusable skeleton components that pulse/shimmer while loading. Apply to tables, cards, and charts.",
                "priority": 4
            },
            {
                "type": "ui_improvement",
                "description": "Create notification toast system in frontend/src/components/ToastNotifications.tsx",
                "context": "Support success/error/warning/info types. Auto-dismiss after 5s. Stack multiple toasts. Slide-in animation from top-right.",
                "priority": 5
            },
            {
                "type": "ui_improvement",
                "description": "Add search filters panel with advanced options in frontend/src/components/SearchFilters.tsx",
                "context": "Include: date range picker, document type selector, tenant filter, sort options. Collapsible panel with clear all button.",
                "priority": 5
            },
            # Backend features
            {
                "type": "optimization",
                "description": "Implement batch embedding generation in app/services/embedding_service.py",
                "context": "Current code embeds one document at a time. Add batch_embed(texts: List[str]) method.",
                "priority": 5
            },
            {
                "type": "monitoring",
                "description": "Add Prometheus metrics for LLM API latency in app/services/model_router.py",
                "context": "Track request_latency_seconds histogram per model. Use prometheus_client library.",
                "priority": 3
            },
        ]

        # Rotate based on iteration
        task_index = self.iteration % len(improvement_tasks)
        return improvement_tasks[task_index]

    async def council_vote(self, task: dict, proposal: dict, metrics: dict) -> dict:
        """Get council consensus on proposal"""
        proposals = []

        # Critic review
        if proposal.get('code'):
            critic_review = await self.critic.review(proposal['code'], task['description'])
            proposals.append({
                "role": "critic",
                "decision": "approve" if critic_review.get("approval") else "reject",
                **critic_review
            })

        # Analyst input
        analyst_input = await self.analyst.analyze(metrics)
        proposals.append({
            "role": "analyst",
            "decision": "approve" if analyst_input.get("health_status") != "critical" else "modify",
            **analyst_input
        })

        # Chairman final decision
        chairman_decision = await self.chairman.decide(task['description'], proposals, {"metrics": metrics})
        proposals.append({
            "role": "chairman",
            **chairman_decision
        })

        # Consensus
        consensus = await reach_consensus(proposals, {"iteration": self.iteration})
        return consensus

    async def execute_task(self, task: dict, proposal: dict) -> bool:
        """Execute approved task"""
        logger.info(f"‚öôÔ∏è Executing: {task['description']}")

        # --- DATA AUGMENTATION TASK ---
        if task.get("type") == "data_augmentation":
            try:
                import httpx
                # Fetch some IDs to augment
                async with self.db_session() as session:
                    from sqlalchemy import text
                    result = await session.execute(text("SELECT id FROM documents LIMIT 20"))
                    ids = [str(r[0]) for r in result.fetchall()]

                if not ids:
                    logger.warning("No documents found for augmentation")
                    return False

                # Call internal API
                async with httpx.AsyncClient() as client:
                    resp = await client.post(
                        "http://backend:8000/api/v1/ml/datasets/generate",
                        json={
                            "document_ids": ids,
                            "method": "synonym",
                            "variations_per_doc": 2
                        },
                        timeout=60.0
                    )

                if resp.status_code == 200:
                    logger.info(f"‚úÖ Data augmentation triggered: {resp.json()}")
                    return True
                else:
                    logger.error(f"‚ùå Augmentation API failed: {resp.text}")
                    return False
            except Exception as e:
                logger.error(f"Augmentation execution failed: {e}")
                return False

        # --- CODE IMPROVEMENT TASK ---
        file_path = proposal.get("file_path")
        code_content = proposal.get("code")

        if not file_path or not code_content:
            logger.warning("Proposed execution missing file_path or code")
            return False

        full_path = os.path.join(PROJECT_ROOT, file_path)

        try:
            # 1. Write Code
            # Ensure directory exists
            os.makedirs(os.path.dirname(full_path), exist_ok=True)

            with open(full_path, "w") as f:
                f.write(code_content)

            logger.info(f"üíæ Wrote code to {full_path}")

            # 2. Commit and Push
            files_changed = [file_path]
            commit_success = await self.git_committer.commit_improvement(
                description=task["description"],
                files_changed=files_changed,
                metadata={
                    "cycle": self.iteration,
                    "improvements": [proposal.get("description", "Code update")],
                    "council_decision": "APPROVED",
                }
            )

            if commit_success:
                await self.notify(f"‚úÖ Changes applied and committed: {file_path}")
                return True
            else:
                await self.notify(f"‚ö†Ô∏è Changes applied but git commit failed")
                return True # Code is there, just git failed

        except Exception as e:
            logger.error(f"Execution failed: {e}")
            await self.notify(f"‚ùå Execution failed: {e}")
            return False

    async def run_guardians(self):
        """Run always-on guardian agents"""
        ui_stop = False
        if self.redis:
            try:
                stop_signal = await self.redis.get(UI_STOP_SIGNAL_KEY)
                ui_stop = stop_signal == "1" if stop_signal else False
            except Exception:
                pass

        if not ui_stop:
            asyncio.create_task(self.ui_guardian.check_ui())

        asyncio.create_task(self.data_sentinel.validate_data())

        # Run Change Observer to detect system changes
        if self.change_observer:
            asyncio.create_task(self._run_change_observation())

        # Run Performance Monitoring
        if self.performance:
            asyncio.create_task(self._run_performance_monitoring())

        # Log system health report
        if self.self_healing:
            health = self.self_healing.get_health_report()
            if health.get("status") != "healthy":
                logger.warning(f"‚ö†Ô∏è System health: {health['status']} (error rate: {health['error_rate']:.2f}/min)")

    async def _run_performance_monitoring(self):
        """Collect and analyze performance metrics"""
        try:
            # Collect metrics
            metrics = await self.gather_metrics()

            for metric_name, value in metrics.items():
                await self.performance.record_metric(metric_name, value)

            # Check for anomalies
            for metric_name in metrics.keys():
                is_anomaly, score = await self.performance.detect_anomaly(metric_name)
                if is_anomaly:
                    logger.warning(f"üî¥ Anomaly: {metric_name} (score: {score:.2f})")

            # Get scaling recommendations
            if self.auto_scaler:
                scaling = await self.auto_scaler.get_scaling_recommendation()
                if scaling.get("action") not in ["none", "scale_down_candidate"]:
                    logger.info(f"üìà Scaling recommendation: {scaling}")

        except Exception as e:
            logger.error(f"Performance monitoring error: {e}")

    async def _run_change_observation(self):
        """Run change observation and arbitration cycle"""
        try:
            result = await self.change_observer.observe()
            if result.get("proposals_generated", 0) > 0:
                logger.info(f"üëÅÔ∏è Change Observer: {result['proposals_generated']} proposals generated")

                # Process pending proposals through arbitration
                await self._process_pending_proposals()
        except Exception as e:
            logger.error(f"Change observation error: {e}")

    async def _process_pending_proposals(self):
        """Process pending proposals through Council arbitration"""
        if not self.redis:
            return

        try:
            pending_json = await self.redis.get("proposals:pending")
            if not pending_json:
                return

            proposals = json.loads(pending_json)
            approved_count = 0

            for proposal in proposals[:3]:  # Process max 3 per cycle
                decision = await self.arbitrator.arbitrate(proposal)
                if decision.get("status") == "approved":
                    approved_count += 1
                    logger.info(f"‚úÖ Proposal approved: {proposal.get('title', 'Unknown')}")
                else:
                    logger.info(f"‚ùå Proposal rejected: {proposal.get('title', 'Unknown')}")

            # Clear processed proposals
            if len(proposals) <= 3:
                await self.redis.delete("proposals:pending")
            else:
                await self.redis.set("proposals:pending", json.dumps(proposals[3:]))

        except Exception as e:
            logger.error(f"Proposal processing error: {e}")

    async def _notify_decision(self, task: str, decision: str, votes: dict, reason: str):
        """Send detailed decision notification to Telegram"""
        if self.telegram and hasattr(self.telegram, 'notify_decision'):
            try:
                await self.telegram.notify_decision(task, decision, votes, reason)
            except Exception as e:
                logger.error(f"Failed to notify decision: {e}")
        logger.info(f"‚öñÔ∏è Council decision: {decision} - {task[:50]}")

    async def notify(self, message: str):
        """Send notification via Telegram"""
        # if self.telegram:
        #    await self.telegram.send_message(message)
        # Using broadcast instead
        pass
        logger.info(f"üì¢ {message}")

async def main():
    orchestrator = AutonomousOrchestrator()
    await orchestrator.initialize()
    await orchestrator.infinite_loop()

if __name__ == "__main__":
    asyncio.run(main())
